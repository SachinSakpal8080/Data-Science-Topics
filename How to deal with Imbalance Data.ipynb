{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imbalanced Dataser\n",
    "When observation in one class is higher than the observation in other classes then there exists a class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Time</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V1</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-1.158233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V2</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>0.877737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V3</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>1.548718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V4</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>0.403034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V5</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>-0.407193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V6</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.095921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V7</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.592941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V8</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-0.270533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V9</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>0.817739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V10</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>0.753074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V11</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>-0.822843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V12</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.538196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V13</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>1.345852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V14</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-1.119670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V15</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>0.175121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V16</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.451449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V17</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>-0.237033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V18</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-0.038195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V19</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>0.803487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V20</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>0.408542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V21</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>-0.009431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V22</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>0.798278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V23</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-0.137458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V24</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.141267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V25</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.206010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V26</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.502292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V27</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.219422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>V28</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.215153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Amount</td>\n",
       "      <td>149.620000</td>\n",
       "      <td>2.690000</td>\n",
       "      <td>378.660000</td>\n",
       "      <td>123.500000</td>\n",
       "      <td>69.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Class</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1           2           3          4\n",
       "Time      0.000000  0.000000    1.000000    1.000000   2.000000\n",
       "V1       -1.359807  1.191857   -1.358354   -0.966272  -1.158233\n",
       "V2       -0.072781  0.266151   -1.340163   -0.185226   0.877737\n",
       "V3        2.536347  0.166480    1.773209    1.792993   1.548718\n",
       "V4        1.378155  0.448154    0.379780   -0.863291   0.403034\n",
       "V5       -0.338321  0.060018   -0.503198   -0.010309  -0.407193\n",
       "V6        0.462388 -0.082361    1.800499    1.247203   0.095921\n",
       "V7        0.239599 -0.078803    0.791461    0.237609   0.592941\n",
       "V8        0.098698  0.085102    0.247676    0.377436  -0.270533\n",
       "V9        0.363787 -0.255425   -1.514654   -1.387024   0.817739\n",
       "V10       0.090794 -0.166974    0.207643   -0.054952   0.753074\n",
       "V11      -0.551600  1.612727    0.624501   -0.226487  -0.822843\n",
       "V12      -0.617801  1.065235    0.066084    0.178228   0.538196\n",
       "V13      -0.991390  0.489095    0.717293    0.507757   1.345852\n",
       "V14      -0.311169 -0.143772   -0.165946   -0.287924  -1.119670\n",
       "V15       1.468177  0.635558    2.345865   -0.631418   0.175121\n",
       "V16      -0.470401  0.463917   -2.890083   -1.059647  -0.451449\n",
       "V17       0.207971 -0.114805    1.109969   -0.684093  -0.237033\n",
       "V18       0.025791 -0.183361   -0.121359    1.965775  -0.038195\n",
       "V19       0.403993 -0.145783   -2.261857   -1.232622   0.803487\n",
       "V20       0.251412 -0.069083    0.524980   -0.208038   0.408542\n",
       "V21      -0.018307 -0.225775    0.247998   -0.108300  -0.009431\n",
       "V22       0.277838 -0.638672    0.771679    0.005274   0.798278\n",
       "V23      -0.110474  0.101288    0.909412   -0.190321  -0.137458\n",
       "V24       0.066928 -0.339846   -0.689281   -1.175575   0.141267\n",
       "V25       0.128539  0.167170   -0.327642    0.647376  -0.206010\n",
       "V26      -0.189115  0.125895   -0.139097   -0.221929   0.502292\n",
       "V27       0.133558 -0.008983   -0.055353    0.062723   0.219422\n",
       "V28      -0.021053  0.014724   -0.059752    0.061458   0.215153\n",
       "Amount  149.620000  2.690000  378.660000  123.500000  69.990000\n",
       "Class     0.000000  0.000000    0.000000    0.000000   0.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('creditcard.csv')\n",
    "data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Class'].value_counts() # Class count\n",
    "# 1 means fraud is detected\n",
    "# 0 means fraun is not detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data['Class'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Imbalance is a common problem in machine learning, especially in classification problems. Imbalance data can hamper our model accuracy big time.\n",
    "\n",
    "Class Imbalance appear in many domains, including:\n",
    "- Fraud detection\n",
    "- Spam filtering\n",
    "- Disease screening\n",
    "- SaaS subscription churn\n",
    "- Advertising click-throughs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning algorithms work best when the number of samples in each class are about equal. This is because most algorithms are designed to maximize accuracy and reduce errors.\n",
    "\n",
    "However, if the data set is imbalance then In such cases, you get a pretty high accuracy just by predicting the majority class, but you fail to capture the minority class, which is most often the point of creating the model in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset of credit card customers where we have to find out whether the credit card transaction was fraudulent or not.\n",
    "\n",
    "But the problem is that fraud transaction is relatively rare, only 0.17% of the transaction is fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of values in each category: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    99.8273\n",
       "1     0.1727\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('% of values in each category: ')\n",
    "round((data['Class'].value_counts('normalize'))*100,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X:  (284807, 27)\n",
      "Shape of Y:  (284807,)\n",
      "(199364, 27)\n",
      "(85443, 27)\n",
      "(199364,)\n",
      "(85443,)\n"
     ]
    }
   ],
   "source": [
    "#lets define x & y for the model \n",
    "X = data.drop(['Time' , 'V15' , 'V22' , 'Class'] , axis = 1)\n",
    "Y = data['Class']\n",
    "print('Shape of X: ' , X.shape)\n",
    "print('Shape of Y: ' , Y.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 25)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(random_state=0)\n",
    "LR.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score:  0.999197447884272\n",
      "Testing Data Score:  0.9991807403766253\n",
      "\n",
      "Confusion Metrix\n",
      "[[85275    10]\n",
      " [   60    98]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85285\n",
      "           1       0.91      0.62      0.74       158\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.95      0.81      0.87     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training Data Score: ', LR.score(x_train, y_train))\n",
    "print('Testing Data Score: ', LR.score(x_test, y_test))\n",
    "\n",
    "print('\\nConfusion Metrix')\n",
    "print(confusion_matrix(y_test, LR.predict(x_test)))\n",
    "\n",
    "print('\\nClassification Report: ')\n",
    "print(classification_report(y_test, LR.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frauds predicted wrongly:  [[0.37974684]]\n",
      "Frauds predicted rightly:  [[0.62025316]]\n"
     ]
    }
   ],
   "source": [
    "print('Frauds predicted wrongly: ',\n",
    "      confusion_matrix(y_test, LR.predict(x_test))[1:,:1]/(confusion_matrix(y_test, LR.predict(x_test))[1:,:1]+\n",
    "                                                           confusion_matrix(y_test, LR.predict(x_test))[1:,1:]))\n",
    "print('Frauds predicted rightly: ',\n",
    "     confusion_matrix(y_test, LR.predict(x_test))[1:,1:]/(confusion_matrix(y_test, LR.predict(x_test))[1:,:1]+\n",
    "                                                           confusion_matrix(y_test, LR.predict(x_test))[1:,1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can check the accuracy score is very high but 37% of the frauds are predicted wrongly.\n",
    "\n",
    "To avoid such scenario, we need to balance the data before using it for the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0: (284315, 31)\n",
      "class 1: (492, 31)\n"
     ]
    }
   ],
   "source": [
    "# class count\n",
    "class_count_0, class_count_1 = data['Class'].value_counts()\n",
    "\n",
    "# Separate class\n",
    "class_0 = data[data['Class'] == 0]\n",
    "class_1 = data[data['Class'] == 1]# print the shape of the class\n",
    "print('class 0:', class_0.shape)\n",
    "print('class 1:', class_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Random Under-Sampling\n",
    "Undersampling can be defined as removing some observations of the majority class. This is done until the majority and minority class is balanced out.\n",
    "\n",
    "Undersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback to undersampling is that we are removing information that may be valuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total class of 1 and 0:\n",
      "1    492\n",
      "0    492\n",
      "Name: Class, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_0_under = class_0.sample(class_count_1)\n",
    "\n",
    "test_under = pd.concat([class_0_under, class_1], axis=0)\n",
    "\n",
    "print(\"total class of 1 and 0:\",)# plot the count after under-sampeling\n",
    "print(test_under['Class'].value_counts())\n",
    "test_under['Class'].value_counts().plot(kind='bar', title='count (target)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Random Over-Sampling\n",
    "Oversampling can be defined as adding more copies to the minority class. Oversampling can be a good choice when you don’t have a ton of data to work with.\n",
    "\n",
    "A con to consider when undersampling is that it can cause overfitting and poor generalization to your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total class of 1 and 0: \n",
      "1    284315\n",
      "0    284315\n",
      "Name: Class, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_1_over = class_1.sample(class_count_0, replace=True)\n",
    "\n",
    "test_over = pd.concat([class_1_over, class_0], axis=0)\n",
    "\n",
    "print(\"total class of 1 and 0: \")# plot the count after under-sampeling\n",
    "print(test_over['Class'].value_counts())\n",
    "test_over['Class'].value_counts().plot(kind='bar', title='count (target)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Random under-sampling with imblearn\n",
    "RandomUnderSampler is a fast and easy way to balance the data by randomly selecting a subset of data for the targeted classes. Under-sample the majority class(es) by randomly picking samples with or without replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset shape: Counter({0: 284315, 1: 492})\n",
      "Resample dataset shape Counter({0: 492, 1: 492})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42, replacement=True)# fit predictor and target variable\n",
    "x_rus, y_rus = rus.fit_resample(X, Y)\n",
    "\n",
    "print('original dataset shape:', Counter(Y))\n",
    "print('Resample dataset shape', Counter(y_rus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Random over-sampling with imblearn\n",
    "One way to fight imbalance data is to generate new samples in the minority classes. The most naive strategy is to generate new samples by randomly sampling with replacement of the currently available samples. The RandomOverSampler offers such a scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({0: 284315, 1: 492})\n",
      "Resample dataset shape Counter({0: 284315, 1: 284315})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# fit predictor and target variable\n",
    "x_ros, y_ros = ros.fit_resample(X, Y)\n",
    "\n",
    "print('Original dataset shape', Counter(Y))\n",
    "print('Resample dataset shape', Counter(y_ros))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Under-sampling: Tomek links\n",
    "Tomek links are pairs of very close instances but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process.\n",
    "\n",
    "Tomek’s link exists if the two samples are the nearest neighbors of each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({0: 284315, 1: 492})\n",
      "Resample dataset shape Counter({0: 284315, 1: 284315})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "tl = RandomOverSampler(sampling_strategy='majority')\n",
    "\n",
    "# fit predictor and target variable\n",
    "x_tl, y_tl = ros.fit_resample(X, Y)\n",
    "\n",
    "print('Original dataset shape', Counter(Y))\n",
    "print('Resample dataset shape', Counter(y_ros))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Synthetic Minority Oversampling Technique (SMOTE)\n",
    "This technique generates synthetic data for the minority class.\n",
    "\n",
    "SMOTE (Synthetic Minority Oversampling Technique) works by randomly picking a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE algorithm works in 4 simple steps:\n",
    "\n",
    "1. Choose a minority class as the input vector\n",
    "2. Find its k nearest neighbors (k_neighbors is specified as an argument in the SMOTE() function)\n",
    "3. Choose one of these neighbors and place a synthetic point anywhere on the line joining the point under consideration and its chosen neighbor\n",
    "4. Repeat the steps until data is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({0: 284315, 1: 492})\n",
      "Resample dataset shape Counter({0: 284315, 1: 284315})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "\n",
    "# fit predictor and target variable\n",
    "x_smote, y_smote = smote.fit_resample(X, Y)\n",
    "\n",
    "print('Original dataset shape', Counter(Y))\n",
    "print('Resample dataset shape', Counter(y_ros))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. NearMiss\n",
    "NearMiss is an under-sampling technique. Instead of resampling the Minority class, using a distance, this will make the majority class equal to the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: Counter({0: 284315, 1: 492})\n",
      "Resample dataset shape: Counter({0: 492, 1: 492})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "nm = NearMiss()\n",
    "\n",
    "x_nm, y_nm = nm.fit_resample(X, Y)\n",
    "\n",
    "print('Original dataset shape:', Counter(Y))\n",
    "print('Resample dataset shape:', Counter(y_nm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
